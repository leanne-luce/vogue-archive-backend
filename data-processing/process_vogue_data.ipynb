{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vogue Archive Data Processing\n",
    "\n",
    "This notebook processes Vogue magazine data and creates vector embeddings for semantic search.\n",
    "\n",
    "**Run this in Google Colab for free GPU access**\n",
    "\n",
    "Runtime: ~20 minutes for 10k records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install sentence-transformers pinecone pandas tqdm pyarrow torch transformers ftfy regex"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pinecone import Pinecone, ServerlessSpec\n\n# Your Pinecone API key\nPINECONE_API_KEY = \"pcsk_2JKS4Y_LNuT72kmgxsuWksy2LyqcQP5Q2iX626vPCwb2KEjj23Vf72a43ZWgNp6FcCJshz\"\nINDEX_NAME = \"vogue-archive-clip\"  # New index name for CLIP embeddings\n\n# Initialize Pinecone\npc = Pinecone(api_key=PINECONE_API_KEY)\n\n# Create index if it doesn't exist\n# CLIP uses 512 dimensions (vs 384 for MiniLM)\nif INDEX_NAME not in pc.list_indexes().names():\n    pc.create_index(\n        name=INDEX_NAME,\n        dimension=512,  # CLIP ViT-B/32 dimension\n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        )\n    )\n\nindex = pc.Index(INDEX_NAME)\nprint(f\"Index '{INDEX_NAME}' ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Download Pre-computed Image Embeddings\n\nThe dataset includes pre-computed CLIP image embeddings - we'll use these!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport requests\n\n# Define the base URL for the archive\nARCHIVE_BASE_URL = \"https://archive.org/download/VogueRunway_dataset\"\n\nprint(\"Downloading pre-computed CLIP image embeddings...\")\nembeddings_url = f\"{ARCHIVE_BASE_URL}/img_emb/VogueRunway_image.npy\"\n\nresponse = requests.get(embeddings_url, stream=True)\nwith open('VogueRunway_image.npy', 'wb') as f:\n    for chunk in response.iter_content(chunk_size=8192):\n        f.write(chunk)\n\n# Load the embeddings\nimage_embeddings = np.load('VogueRunway_image.npy')\nprint(f\"✓ Loaded {len(image_embeddings):,} pre-computed CLIP image embeddings\")\nprint(f\"Embedding dimension: {image_embeddings.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Load Vogue Runway Metadata\n\nDownload metadata and match with embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport pandas as pd\n\nprint(\"Downloading Vogue Runway metadata...\")\nurl = f\"{ARCHIVE_BASE_URL}/VogueRunway.parquet\"\n\n# Download parquet file\nresponse = requests.get(url, stream=True)\nwith open('VogueRunway.parquet', 'wb') as f:\n    for chunk in response.iter_content(chunk_size=8192):\n        f.write(chunk)\n\nprint(\"Loading metadata...\")\ndf = pd.read_parquet('VogueRunway.parquet')\n\nprint(f\"Total items: {len(df):,}\")\nprint(f\"Total embeddings: {len(image_embeddings):,}\")\n\n# Take top items by aesthetic score (you can change 1000 to 10000 or 100000)\nNUM_ITEMS = 1000\n\nif 'aesthetic' in df.columns:\n    df = df.nlargest(NUM_ITEMS, 'aesthetic')\n    print(f\"\\nSelected top {NUM_ITEMS} items by aesthetic score\")\nelse:\n    df = df.head(NUM_ITEMS)\n    print(f\"\\nTaking first {NUM_ITEMS} items\")\n\n# Reset index to get clean indices\ndf = df.reset_index(drop=True)\n\nprint(f\"\\nSample data:\")\nprint(df[['key', 'designer', 'season', 'year', 'category', 'city']].head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Match Embeddings with Metadata and Upload to Pinecone\n\nUse pre-computed image embeddings for multimodal search"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm import tqdm\n\n# Batch size for uploading\nBATCH_SIZE = 100\n\ndef process_batch(batch_df, batch_embeddings):\n    \"\"\"Process a batch of records with pre-computed embeddings\"\"\"\n    vectors = []\n    \n    for idx, row in batch_df.iterrows():\n        # Get the pre-computed embedding for this item\n        # The embedding index should match the row index in the original dataset\n        embedding_idx = row['key']  # Use the key to find the right embedding\n        \n        # Get embedding (already computed from images!)\n        embedding = batch_embeddings[idx].tolist()\n        \n        # Prepare metadata\n        metadata = {\n            \"description\": f\"{row.get('designer', '')} {row.get('season', '')} {row.get('year', '')} {row.get('category', '')} {row.get('section', '')}\".strip(),\n            \"designer\": str(row.get('designer', '')),\n            \"season\": str(row.get('season', '')),\n            \"year\": int(row.get('year', 0)) if pd.notna(row.get('year')) else 0,\n            \"category\": str(row.get('category', '')),\n            \"city\": str(row.get('city', '')),\n            \"section\": str(row.get('section', '')),\n            \"image_url\": row.get('url', ''),\n            \"aesthetic_score\": float(row.get('aesthetic', 0)) if pd.notna(row.get('aesthetic')) else 0,\n        }\n        \n        vectors.append({\n            \"id\": f\"vogue_runway_{row['key']}\",\n            \"values\": embedding,\n            \"metadata\": metadata\n        })\n    \n    # Upload to Pinecone\n    index.upsert(vectors=vectors)\n    return len(vectors)\n\n# Get embeddings for selected items\n# Map each row's key to its embedding\nselected_embeddings = []\nfor _, row in df.iterrows():\n    key = row['key']\n    # The embeddings array is ordered by key\n    selected_embeddings.append(image_embeddings[key])\n\nselected_embeddings = np.array(selected_embeddings)\n\nprint(f\"\\nProcessing {len(df)} items with pre-computed embeddings in batches of {BATCH_SIZE}...\")\ntotal_uploaded = 0\n\nfor i in tqdm(range(0, len(df), BATCH_SIZE)):\n    batch_df = df.iloc[i:i+BATCH_SIZE]\n    batch_emb = selected_embeddings[i:i+BATCH_SIZE]\n    count = process_batch(batch_df, batch_emb)\n    total_uploaded += count\n\nprint(f\"\\n✓ Upload complete! {total_uploaded} vectors uploaded to Pinecone.\")\nprint(f\"\\nIndex stats: {index.describe_index_stats()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Test Multimodal Search\n\nTest text queries against image embeddings - this is CLIP's superpower!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sentence_transformers import SentenceTransformer\n\n# Load CLIP text encoder for queries\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n# Test queries\ntest_queries = [\n    \"elegant evening gown\",\n    \"minimalist black dress\",\n    \"tweed jacket\",\n    \"vintage cocktail dress\"\n]\n\nfor query in test_queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"Query: '{query}'\")\n    print(f\"{'='*60}\")\n    \n    # Encode text query with CLIP\n    query_embedding = model.encode(query).tolist()\n    \n    # Search against image embeddings\n    results = index.query(\n        vector=query_embedding,\n        top_k=3,\n        include_metadata=True\n    )\n    \n    for i, match in enumerate(results['matches'], 1):\n        print(f\"\\n{i}. Score: {match['score']:.3f}\")\n        print(f\"   Designer: {match['metadata']['designer']}\")\n        print(f\"   {match['metadata']['season']} {match['metadata']['year']}\")\n        print(f\"   Category: {match['metadata']['category']}\")\n        print(f\"   City: {match['metadata']['city']}\")\n        if match['metadata'].get('image_url'):\n            print(f\"   Image: {match['metadata']['image_url'][:80]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done! Multimodal Search Ready\n\nYour Vogue archive now uses **image embeddings** in the database.\n\nWhen users search with text, CLIP matches:\n- Text query → Image embeddings\n- This finds visually similar runway looks based on semantic understanding\n\n**Benefits:**\n✓ Faster processing (no embedding generation needed)\n✓ Better visual understanding (searches actual image features)\n✓ True multimodal CLIP search (text-to-image matching)\n\nNext steps:\n1. Deploy the API (see ../api/)\n2. Your React Native app is already configured!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}